Challenges Faced â€“ Mall Customer Segmentation Project
This document summarizes the main technical challenges faced and how they were resolved during the development of a PySpark-based data analysis project inside a WSL environment.

Environment Setup
Ubuntu 24.04 came with Python 3.12, which caused PySpark compatibility issues.
Solution: Installed Python 3.11.8 using pyenv and created a virtual environment with venv.

PySpark Import Errors
ModuleNotFoundError when importing PySpark in notebooks.
Solution: Installed pyspark and findspark inside the virtual environment and used findspark.init() to register Spark.

VSCode Exiting WSL Mode
VSCode switched out of WSL when opening local files.
Solution: Always launched the project from inside WSL using code . to stay in Linux mode.

Jupyter Kernel Mismatch
Notebooks defaulted to wrong Python (e.g., Windows or Python 3.12rc).
Solution: Manually selected the correct kernel: Python 3.11.8 from WSL venv.

File Access Performance
Reading files from /mnt/c/ was slower in Spark.
Solution: Continued using it for now, but noted /home/username/ is better for big data processing.

Hadoop Native Warnings
Spark showed native Hadoop library warnings.
Solution: Ignored as these are safe in local mode.

Outcome
All issues were successfully resolved. The system now runs:
- PySpark 3.5.5
- Inside WSL (Ubuntu 24.04)
- With a Jupyter notebook using the correct Python 3.11.8 venv

The environment is stable and ready for further Spark-based analysis and clustering.